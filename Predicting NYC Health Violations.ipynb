{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-17T01:03:10.893420Z",
     "start_time": "2022-05-17T01:03:10.861202Z"
    }
   },
   "source": [
    "https://www.nycfoodpolicy.org/restaurant-grading-system-hunter-college-nyc-food-policy-center/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cities across the United States are capitalizing on big data. Predictive policing is becoming a prominent tool for public safety in many cities. In Boston, an algorithm helps determine “problem properties” where the city can target interventions. In Chicago, they are protecting citizens by predicting which landlords are not complying with city ordinances. In New York, the Fire Department sends inspectors to the highest risk buildings so they can prevent deadly fires from breaking out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the CDC, more than 48 million Americans per year become sick from food, and an estimated 75% of the outbreaks came from food prepared by caterers, delis, and restaurants. In most cities, health inspections are generally random, which can increase time spent on spot checks at clean restaurants that have been following the rules closely — and missed opportunities to improve health and hygiene at places with more pressing food safety issues.\n",
    "\n",
    "The goal for this project is to leverage public citizen generated data from social media to narrow the search for critical health and safety violations in New York City. As the City of New York manages  an open data portal, everyone can access historical hygiene inspections and violation records. By combine these two data source this project aims to determine which words, phrases, ratings, and patterns among restaurants lead to critical health and safety violations. This model can assist city health inspectors do their job better by prioritizing the kitchens most likely to be in violation of code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The New York Health Department inspects the approximately 27,000 restaurants within the city to monitor their compliance with food safety regulations. Inspectors observe how food is prepared, served and stored and whether restaurant workers are practicing good hygiene. They check food temperatures, equipment maintenance and pest control measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T22:28:37.643486Z",
     "start_time": "2022-07-25T22:26:40.350363Z"
    }
   },
   "outputs": [],
   "source": [
    "from IPython import display\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "import plotly.express as px\n",
    "\n",
    "import warnings\n",
    "import time\n",
    "\n",
    "import folium\n",
    "import folium.plugins as plugins\n",
    "\n",
    "# misc\n",
    "import glob, os\n",
    "import ast\n",
    "\n",
    "call_apis = False\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pd.set_option('display.max_colwidth',200)\n",
    "pd.set_option('display.max_columns',50)\n",
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "#NLP \n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "from spacy import displacy\n",
    "import nltk \n",
    "import string\n",
    "from nltk.collocations import *\n",
    "from nltk import word_tokenize,wordpunct_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from wordcloud import WordCloud\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "\n",
    "# sk-learn\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer, CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can you predict if a restaurant has received a C Grade?\n",
    "### Can you predict a restaurants yelp rating based on its review text?\n",
    "### Can you recommend a similar restaurant nearby with a better health score for restaurants searched for with a C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this project there will be two sources and types of data used:\n",
    "\n",
    "* Historical health and hygiene inspections recorded by New York City Department of Health and Mental Hygiene (DOHMH) public health inspectors\n",
    "* User generated Yelp business ratings and reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project requires data pulled from two different sources, the City of New York and Yelp. To obtain the data we will call the API keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " > The dataset contains every sustained or not yet adjudicated violation citation from every full or special program inspection conducted up to three years prior to the most recent inspection for restaurants and college cafeterias in an active status on the RECORD DATE (date of the data pull). When an inspection results in more than one violation, values for associated fields are repeated for each additional violation record. Establishments are uniquely identified by their CAMIS (record ID) number. Keep in mind that thousands of restaurants start business and go out of business every year; only restaurants in an active status are included in the dataset.\n",
    "Records are also included for each restaurant that has applied for a permit but has not yet been inspected and for inspections resulting in no violations. Establishments with inspection date of 1/1/1900 are new establishments that have not yet received an inspection. Restaurants that received no violations are represented by a single row and coded as having no violations using the ACTION field.\n",
    "Because this dataset is compiled from several large administrative data systems, it contains some illogical values that could be a result of data entry or transfer errors. Data may also be missing.\n",
    "This dataset and the information on the Health Department’s Restaurant Grading website come from the same data source. The Health Department’s Restaurant Grading website is here:\n",
    "http://www1.nyc.gov/site/doh/services/restaurant-grades.page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Why does the Health Department inspect restaurants?\n",
    "The Health Department inspects the approximately 27,000 restaurants in New York City to monitor their compliance with food safety regulations. Inspectors observe how food is prepared, served and stored and whether restaurant workers are practicing good hygiene. They check food temperatures, equipment maintenance and pest control measures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Since 2010, New York City has required restaurants to post letter grades that correspond to scores received from sanitary inspections. An inspection score of 0 to 13 is an A, 14 to 27 points is a B, and 28 or more points is a C. Grade cards must be posted where they can easily be seen by people passing by."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The New York City Health Department inspects all food service establishments to make sure they meet Health Code requirements, which helps prevent\n",
    "foodborne illness. How often a restaurant is inspected depends on its inspection score. Restaurants that receive a low score on the initial or first inspection\n",
    "in the inspection cycle are inspected less often than those that receive a high score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The points for a particular violation depend on the health risk it poses to the public. Violations fall into three categories:\n",
    ">* A public health hazard, such as failing to keep food at the right temperature, triggers a minimum of 7 points. If the violation can’t be corrected before the inspection ends, the Health Department may close the restaurant until it’s fixed.\n",
    ">* A critical violation, for example, serving raw food such as a salad without properly washing it first, carries a minimum of 5 points.\n",
    ">* A general violation, such as not properly sanitizing\n",
    "cooking utensils, receives at least 2 points.\n",
    "\n",
    ">Inspectors assign additional points to reflect the extent of the\n",
    "violation. A violation’s condition level can range from 1 (least\n",
    "extensive) to 5 (most extensive). For example, the presence of\n",
    "one contaminated food item is a condition level 1 violation,\n",
    "generating 7 points. Four or more contaminated food items\n",
    "is a condition level 4 violation, resulting in 10 points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How are restaurants graded?\n",
    "Violations found during inspections carry point values, and a restaurant’s score corresponds to a letter grade. The point/grade cut-offs are the same as for mobile food vending letter grading, with fewer points corresponding to a better grade:\n",
    "\n",
    ">* \"A\" grade: 0 to 13 points for sanitary violations\n",
    ">* \"B\" grade: 14 to 27 points for sanitary violations\n",
    ">* \"C\" grade: 28 or more points for sanitary violations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The City of New York inspects all restaurants cyclically. And if a business does not pass it's initial inspection for the cycle, it will be re-inspected in 3-5 months."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Restaurant Inspection Results from NYC Open Data Portal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-19T15:04:10.996002Z",
     "start_time": "2022-04-19T15:04:10.983796Z"
    }
   },
   "source": [
    "The dataset can be obtained here\n",
    "\n",
    "https://data.cityofnewyork.us/Health/DOHMH-New-York-City-Restaurant-Inspection-Results/43nn-pn8j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset was downloaded and saved to this repository. Let's load it in and explore its contents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailed descriptions about each column can be found in the Restaurant Inspection Data Dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-25T22:32:46.091974Z",
     "start_time": "2022-07-25T22:32:44.406767Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_df = pd.read_csv('data/nyc_open_data/DOHMH_New_York_City_Restaurant_Inspection_Results.csv')\n",
    "doh_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 186,227 inspection results. However, when an inspection results in more than one violation, values for associated fields are repeated for each additional violation record. So let's check how many individual restaurants are in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding NYC DOHMH Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:16:58.508648Z",
     "start_time": "2022-07-26T17:16:58.064906Z"
    }
   },
   "outputs": [],
   "source": [
    "# How many unique restaurants are in this dataset?\n",
    "n_unique = doh_df['CAMIS'].nunique()\n",
    "print(f'There are {n_unique} unique restaurants in the dataset. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:01.948239Z",
     "start_time": "2022-07-26T17:16:59.935858Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get more information about the dataset contents\n",
    "doh_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:05.758296Z",
     "start_time": "2022-07-26T17:17:05.478066Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_df['ZIPCODE'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:07.184504Z",
     "start_time": "2022-07-26T17:17:06.740663Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_df['Community Board'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:09.763465Z",
     "start_time": "2022-07-26T17:17:09.561104Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_df['Council District'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:11.037528Z",
     "start_time": "2022-07-26T17:17:10.711857Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_df['Census Tract'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every row has a restaurant ID, address, date, and score. Let's ensure there aren't any duplicated rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:14.548543Z",
     "start_time": "2022-07-26T17:17:13.460254Z"
    }
   },
   "outputs": [],
   "source": [
    "print(f'There are {doh_df.duplicated(keep=False).sum()} duplicated rows. ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:17.434765Z",
     "start_time": "2022-07-26T17:17:16.399041Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's drop these duplucated rows\n",
    "doh_df.drop_duplicates(keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:18.470141Z",
     "start_time": "2022-07-26T17:17:18.423338Z"
    }
   },
   "outputs": [],
   "source": [
    "# Confirming duplicates have been removed\n",
    "doh_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this project will be leveraging data publicly generated from social media a lookup value will be needed to call the API and join the tables. The Yelp API has an endpoint for Phone Search. This will allow us to pull Yelp business data for each restaurant by proivinging a telephone number. More infomration can be found in the [documentation here.](https://www.yelp.com/developers/documentation/v3/business_search_phone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:21.209256Z",
     "start_time": "2022-07-26T17:17:21.154982Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking that every restaunt has a phone number\n",
    "missing_num = doh_df['PHONE'].isna().sum()\n",
    "print(f'There are {missing_num} restaunts missing a telephone number.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:22.043298Z",
     "start_time": "2022-07-26T17:17:21.854498Z"
    }
   },
   "outputs": [],
   "source": [
    "# Since only 13 numbers are missing, these rows can be dropped\n",
    "doh_df.dropna(subset=['PHONE'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:23.552372Z",
     "start_time": "2022-07-26T17:17:23.514468Z"
    }
   },
   "outputs": [],
   "source": [
    "# Confirming records were dropped\n",
    "doh_df['PHONE'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:24.890753Z",
     "start_time": "2022-07-26T17:17:24.856167Z"
    }
   },
   "outputs": [],
   "source": [
    "# How many unique restaurants are remaining?\n",
    "n_unique = doh_df['CAMIS'].nunique()\n",
    "print(f'There are {n_unique} unique restaurants remaining in the dataset. ')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explore the date range for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:28.178826Z",
     "start_time": "2022-07-26T17:17:27.827219Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_df['INSPECTION DATE'] =  pd.to_datetime(doh_df['INSPECTION DATE'])\n",
    "begin_date = doh_df['INSPECTION DATE'].min()\n",
    "end_date = doh_df['INSPECTION DATE'].max()\n",
    "print(f'The data ranges from {begin_date} to {end_date}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspections in this dataset range from May 2009 up to March 2022."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Target Variable -- NYCDOH Inspection Grades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Health code violations found during an inspections carries a point value, and a restaurant’s score corresponds to a letter grade. A lower point score, leads to a better letter grade:\n",
    "\n",
    "* \"A\" grade: 0 to 13 points for sanitary violations\n",
    "* \"B\" grade: 14 to 27 points for sanitary violations\n",
    "* \"C\" grade: 28 or more points for sanitary violations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:34.323595Z",
     "start_time": "2022-07-26T17:17:32.379355Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let see what the score distribution is\n",
    "doh_df['SCORE'].hist(bins=113, figsize=(12,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:38.293655Z",
     "start_time": "2022-07-26T17:17:38.264445Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_df['SCORE'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:42.029955Z",
     "start_time": "2022-07-26T17:17:41.892940Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_graded = doh_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:50.319783Z",
     "start_time": "2022-07-26T17:17:50.264248Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_graded.drop(columns=['INSPECTION DATE', 'ACTION', 'VIOLATION CODE',\n",
    "       'VIOLATION DESCRIPTION', 'CRITICAL FLAG','GRADE',\n",
    "       'GRADE DATE', 'RECORD DATE', 'INSPECTION TYPE'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:51.411401Z",
     "start_time": "2022-07-26T17:17:51.386988Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_graded['A'] = (doh_graded['SCORE'] < 14).astype(int)\n",
    "doh_graded['B'] = (doh_graded['SCORE'] > 13).astype(int) & (doh_df['SCORE'] < 28).astype(int)\n",
    "doh_graded['C'] = (doh_graded['SCORE'] > 27).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:56.764519Z",
     "start_time": "2022-07-26T17:17:52.344818Z"
    }
   },
   "outputs": [],
   "source": [
    "doh_grouped = doh_graded.groupby(by=['CAMIS', 'DBA','CUISINE DESCRIPTION',\n",
    "                                     'BORO', 'BUILDING',\n",
    "                                     'STREET', 'ZIPCODE', 'PHONE', 'Latitude',\n",
    "                                     'Longitude', 'Community Board',\n",
    "                                     'Council District','Census Tract'],dropna=False)['A','B','C'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:17:59.522015Z",
     "start_time": "2022-07-26T17:17:59.513481Z"
    }
   },
   "outputs": [],
   "source": [
    "(doh_grouped['B'] > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 19,790 unique restaurants, 9,977 failed an initial cycle inspection at least once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:18:03.677368Z",
     "start_time": "2022-07-26T17:18:03.650324Z"
    }
   },
   "outputs": [],
   "source": [
    "(doh_grouped['C'] > 0).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the 19,790 unique restaurants, 5,648 severly failed an initial cycle inspection at least once and are at risk of being closed by the DOHMH."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:18:08.791428Z",
     "start_time": "2022-07-26T17:18:08.782910Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating the Target Variable 'Severe' for Restaurants that have scored over 28 points in an initial inspection.\n",
    "doh_grouped['Severe'] = (doh_grouped['C'] > 0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:18:10.208796Z",
     "start_time": "2022-07-26T17:18:10.091267Z"
    }
   },
   "outputs": [],
   "source": [
    "nyc_df = doh_grouped.reset_index()\n",
    "nyc_df.drop(['A','B','C'],axis=1, inplace=True)\n",
    "nyc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NYC DOH Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:18:23.148642Z",
     "start_time": "2022-07-26T17:18:23.043753Z"
    }
   },
   "outputs": [],
   "source": [
    "nyc_df['BORO'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:18:24.287841Z",
     "start_time": "2022-07-26T17:18:24.272959Z"
    }
   },
   "outputs": [],
   "source": [
    "nyc_df['CUISINE DESCRIPTION'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:18:34.903453Z",
     "start_time": "2022-07-26T17:18:34.853755Z"
    }
   },
   "outputs": [],
   "source": [
    "nyc_df['ZIPCODE'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Obtaining Yelp Buniness and Review Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an understanding of the city's inspection results and have explored that dataset it is time to pull in data from the crowd-sourced review platform Yelp."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:18:44.900657Z",
     "start_time": "2022-07-26T17:18:44.813972Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loading in locally stored API credentials. \n",
    "# You can sign up for access and obtain credentials to the Yelp API here: \n",
    "# https://www.yelp.com/developers/documentation/v3\n",
    "\n",
    "with open('/Users/Rob/.secret/yelp_api.json') as f:\n",
    "    creds = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:18:46.537137Z",
     "start_time": "2022-07-26T17:18:46.526933Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking creds were properly loaded in\n",
    "creds.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Business Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:18:49.811744Z",
     "start_time": "2022-07-26T17:18:49.790854Z"
    }
   },
   "outputs": [],
   "source": [
    "# Formatting phone numbers provided in the NYCDOH dataset \n",
    "nyc_df['PHONE'] = '+1'+nyc_df['PHONE']\n",
    "\n",
    "# Ensure the list contains unique phone numbers only\n",
    "phone_numbers = set(nyc_df['PHONE'])\n",
    "phone_numbers = list(phone_numbers)\n",
    "number_count = len(phone_numbers)\n",
    "print(f'There are {number_count} unique phone numbers.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we will call the Yelp API at the Phone Search Endpoint for all the numbers in the `phone_numbers` list. However the API only allows 5000 callers per day so we'll slice the list into smaller list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T21:10:36.638946Z",
     "start_time": "2022-05-16T21:10:36.634821Z"
    }
   },
   "outputs": [],
   "source": [
    "# Slicing the phone list into smaller list to fit under the API daily limit restrictions\n",
    "# phone_numbers1 = phone_numbers[1:1000]\n",
    "# phone_numbers2 = phone_numbers[1000:2000]\n",
    "# phone_numbers3 = phone_numbers[2000:2500]\n",
    "# phone_numbers4 = phone_numbers[2500:3500]\n",
    "# phone_numbers5 = phone_numbers[3500:5000]\n",
    "# phone_numbers6 = phone_numbers[5000:6000]\n",
    "# phone_numbers7 = phone_numbers[6000:7500]\n",
    "# phone_numbers8 = phone_numbers[7500:10000]\n",
    "# phone_numbers9 = phone_numbers[10000:12500]\n",
    "# phone_numbers11 = phone_numbers[15000:17500]\n",
    "# phone_numbers12 = phone_numbers[17500:20000]\n",
    "# phone_numbers9 = phone_numbers[10000:12500]\n",
    "# phone_numbers10 = phone_numbers[12500:15000]\n",
    "# phone_numbers11 = phone_numbers[15000:17500]\n",
    "# phone_numbers12 = phone_numbers[17500:19000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T21:10:37.297919Z",
     "start_time": "2022-05-16T21:10:37.293660Z"
    }
   },
   "outputs": [],
   "source": [
    "# #Functionizing the Yelp API Phone Search\n",
    "\n",
    "# def get_businesses(phone_numbers):\n",
    "#     \"\"\"Input a list of formatted phone numbers\n",
    "#     (must start with + and include the country code, like +14159083801)\n",
    "#     and returns a corresponding list of Yelp Businesses\"\"\"\n",
    "    \n",
    "#     biz_list = []\n",
    "    \n",
    "#     for number in phone_numbers:\n",
    "#         url = 'https://api.yelp.com/v3/businesses/search/phone'\n",
    "#         headers = {'Authorization': 'Bearer ' + creds['api_key']}\n",
    "#         url_params = {'phone': number}\n",
    "#         response = requests.get(url, headers=headers, params=url_params)\n",
    "#         response_json = response.json()\n",
    "#         biz_list.extend(response_json.get('businesses','U'))\n",
    "        \n",
    "#     while 'U' in biz_list:\n",
    "#         biz_list.remove('U')\n",
    "        \n",
    "#     return biz_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T21:10:38.308764Z",
     "start_time": "2022-05-16T21:10:38.304543Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Call `get_business` function\n",
    "# if call_apis == True:\n",
    "#     biz_list12 = get_businesses(phone_numbers12)\n",
    "    \n",
    "#     # Save returned list as a DataFrame and .csv file\n",
    "#     biz12_df = pd.DataFrame(biz_list12)\n",
    "#     biz12_df.to_csv('data/yelp_data/yelp_business/yelp_phone12.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:19:05.127529Z",
     "start_time": "2022-07-26T17:19:05.103470Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of files containing Yelp business data\n",
    "fpath = 'data/yelp_data/yelp_businesses/'\n",
    "os.listdir(fpath)\n",
    "query = fpath+\"*.csv\"\n",
    "f_list = glob.glob(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:19:10.434481Z",
     "start_time": "2022-07-26T17:19:09.534109Z"
    }
   },
   "outputs": [],
   "source": [
    "# Append saved Yelp Business tables to a dict\n",
    "yelp_tables = {}\n",
    "\n",
    "for f in f_list:\n",
    "    temp_df = pd.read_csv(f)\n",
    "    fname = f.replace('data/yelp_data/yelp_businesses/yelp_phone','df_').replace('.csv','')\n",
    "    yelp_tables[fname] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:19:11.692278Z",
     "start_time": "2022-07-26T17:19:11.681233Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_df_list = [t for t in list(yelp_tables.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:19:12.818332Z",
     "start_time": "2022-07-26T17:19:12.746323Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenating all Yelp Businesses responses from the Phone Search\n",
    "yelp_businesses_df = pd.concat(yelp_tables,ignore_index=True)\n",
    "yelp_businesses_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exploring Yelp Businesses Response Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:19:21.285451Z",
     "start_time": "2022-07-26T17:19:21.178064Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_businesses_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:19:24.336672Z",
     "start_time": "2022-07-26T17:19:24.304394Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_businesses_df['price'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:19:25.553993Z",
     "start_time": "2022-07-26T17:19:25.472940Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_businesses_df['rating'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:19:27.181092Z",
     "start_time": "2022-07-26T17:19:27.071257Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_businesses_df['categories'].value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:19:40.428026Z",
     "start_time": "2022-07-26T17:19:40.305470Z"
    }
   },
   "outputs": [],
   "source": [
    "# Duplicates?\n",
    "yelp_businesses_df[yelp_businesses_df.duplicated(['id'], keep=False)].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:19:43.708821Z",
     "start_time": "2022-07-26T17:19:43.553961Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_businesses_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:19:44.950417Z",
     "start_time": "2022-07-26T17:19:44.935721Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_businesses_df['review_count'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Yelp Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that all the restaurants from the NYC DOHMH dataset have been used to search the Yelp API and have been concatenated we can use the return url to gather reviews for each business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-16T21:10:56.175141Z",
     "start_time": "2022-05-16T21:10:56.171856Z"
    }
   },
   "outputs": [],
   "source": [
    "# OLD from webscraping\n",
    "# df_10_2 = df_10.loc[1000:2173]\n",
    "# df_10_2.to_csv('df_10_2',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:20:04.782273Z",
     "start_time": "2022-07-26T17:20:04.756637Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_text(url_list):\n",
    "    \"\"\" Given a list of urls, this function will iterate through the list and \n",
    "    extract text from the first page of reviews. The data will be joined into \n",
    "    a corpus for each business\"\"\"\n",
    "    \n",
    "    review_txt = []\n",
    "\n",
    "    for url in url_list:\n",
    "        req = requests.get(url,allow_redirects=False)\n",
    "        soup = bs(req.content)\n",
    "        comments = soup.find_all(class_='raw__09f24__T4Ezm', lang=\"en\")\n",
    "        comment_txt = []\n",
    "\n",
    "        for comment in comments:\n",
    "            comment_txt.append(comment.text)\n",
    "\n",
    "        comment_corp = ('.'.join(comment_txt))\n",
    "        review_txt.append(comment_corp)\n",
    "    return review_txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the time required to run this function, it can be broken into smaller requests. Since we already have smaller list used when we called the API earlier we can take the urls returned from the API here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:20:12.396271Z",
     "start_time": "2022-07-26T17:20:12.378977Z"
    }
   },
   "outputs": [],
   "source": [
    "# Obtaining list of yelp business urls from saved API response\n",
    "url_list = list(yelp_tables['df_1']['url'])\n",
    "len(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:20:14.571551Z",
     "start_time": "2022-07-26T17:20:14.564328Z"
    }
   },
   "outputs": [],
   "source": [
    "# Calling `get_text` function to obtain Yelp reviews\n",
    "if call_apis == True:\n",
    "    review_text = get_text(url_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:20:16.314651Z",
     "start_time": "2022-07-26T17:20:16.287997Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the Reviews to a csv in the repository\n",
    "if call_apis == True:\n",
    "    rvw_txt = pd.DataFrame(review_text,columns=['Review_Text'])\n",
    "    rvw_txt.to_csv('rvw_txt1.csv',index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat this for all the urls returned from the API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining Yelp Reviews to Yelp Business Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:20:19.958113Z",
     "start_time": "2022-07-26T17:20:19.949230Z"
    }
   },
   "outputs": [],
   "source": [
    "# List of files containing Yelp business data\n",
    "fpath = 'data/yelp_data/yelp_reviews/'\n",
    "os.listdir(fpath)\n",
    "query = fpath+\"*.csv\"\n",
    "f_list = glob.glob(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:20:40.148580Z",
     "start_time": "2022-07-26T17:20:38.326508Z"
    }
   },
   "outputs": [],
   "source": [
    "# Append saved Yelp Reviews to a dict\n",
    "rvw_tables = {}\n",
    "\n",
    "for file in f_list:\n",
    "    temp_df = pd.read_csv(file)\n",
    "    fname = file.replace('data/yelp_data/yelp_reviews/rvw_txt','txt_').replace('.csv','')\n",
    "    rvw_tables[fname] = temp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:20:52.363921Z",
     "start_time": "2022-07-26T17:20:52.356362Z"
    }
   },
   "outputs": [],
   "source": [
    "rvw_df_list = [t for t in list(rvw_tables.keys())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:20:53.448577Z",
     "start_time": "2022-07-26T17:20:53.344277Z"
    }
   },
   "outputs": [],
   "source": [
    "rvw1 = pd.read_csv('data/yelp_data/yelp_reviews/rvw_txt1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:21:20.592647Z",
     "start_time": "2022-07-26T17:21:20.584715Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_tables.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:21:21.859426Z",
     "start_time": "2022-07-26T17:21:21.827729Z"
    }
   },
   "outputs": [],
   "source": [
    "rvw_tables.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:21:25.256056Z",
     "start_time": "2022-07-26T17:21:25.213951Z"
    }
   },
   "outputs": [],
   "source": [
    "# Adding review text to the Yelp Business tables\n",
    "yelp_tables['df_1']['Reviews'] =  rvw_tables['txt_1']\n",
    "yelp_tables['df_2']['Reviews'] =  rvw_tables['txt_2']\n",
    "yelp_tables['df_3']['Reviews'] =  rvw_tables['txt_3']\n",
    "yelp_tables['df_4']['Reviews'] =  rvw_tables['txt_4']\n",
    "yelp_tables['df_5']['Reviews'] =  rvw_tables['txt_5']\n",
    "yelp_tables['df_6']['Reviews'] =  rvw_tables['txt_6']\n",
    "yelp_tables['df_7']['Reviews'] =  rvw_tables['txt_7']\n",
    "yelp_tables['df_8']['Reviews'] =  rvw_tables['txt_8']\n",
    "yelp_tables['df_9']['Reviews'] =  rvw_tables['txt_9']\n",
    "yelp_tables['df_10']['Reviews'] =  rvw_tables['txt_10']\n",
    "yelp_tables['df_11']['Reviews'] =  rvw_tables['txt_11']\n",
    "yelp_tables['df_12']['Reviews'] =  rvw_tables['txt_12']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:21:26.667980Z",
     "start_time": "2022-07-26T17:21:26.543162Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenating all Yelp businesses tables with review text included\n",
    "yelp_df = pd.concat(yelp_tables,ignore_index=True)\n",
    "yelp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-04-14T15:07:46.486247Z",
     "start_time": "2022-04-14T15:07:46.435062Z"
    }
   },
   "outputs": [],
   "source": [
    "# OLD Yelp Review API\n",
    "# review_df = pd.DataFrame(response_json.get('reviews'))\n",
    "# review_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joining NYC DOHMH & Yelp Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:21:32.102226Z",
     "start_time": "2022-07-26T17:21:32.048538Z"
    }
   },
   "outputs": [],
   "source": [
    "nyc_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:21:35.092799Z",
     "start_time": "2022-07-26T17:21:34.957829Z"
    }
   },
   "outputs": [],
   "source": [
    "nyc_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:21:36.223890Z",
     "start_time": "2022-07-26T17:21:36.152589Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:21:40.722009Z",
     "start_time": "2022-07-26T17:21:40.639161Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:21:41.932849Z",
     "start_time": "2022-07-26T17:21:41.913626Z"
    }
   },
   "outputs": [],
   "source": [
    "# Formatting Yelp phone numbers to align with NYC phone numbers to join on\n",
    "yelp_df['phone'] = '+' + yelp_df['phone'].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:21:43.898514Z",
     "start_time": "2022-07-26T17:21:43.866577Z"
    }
   },
   "outputs": [],
   "source": [
    "yelp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:21:45.669771Z",
     "start_time": "2022-07-26T17:21:45.547958Z"
    }
   },
   "outputs": [],
   "source": [
    "# Merging NYC and Yelp datasets\n",
    "df_1 = pd.merge(nyc_df, yelp_df, left_on='PHONE', right_on='phone', how='inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:21:47.150434Z",
     "start_time": "2022-07-26T17:21:47.048873Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Saving merged dataset to csv\n",
    "# df_1.to_csv('full_dataset.csv')\n",
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:22:01.040013Z",
     "start_time": "2022-07-26T17:22:00.873323Z"
    }
   },
   "outputs": [],
   "source": [
    "df_1.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrubbing The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:22:05.116778Z",
     "start_time": "2022-07-26T17:22:05.112335Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get missing reviews\n",
    "# Drop businesses missing reviews\n",
    "# Drop unneccessary columns\n",
    "# Check duplicates\n",
    "# ??Feature Engineering\n",
    "# Get coordinates\n",
    "# Get takeout/delivery\n",
    "# Get $$$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:22:07.772839Z",
     "start_time": "2022-07-26T17:22:07.670152Z"
    }
   },
   "outputs": [],
   "source": [
    "# Getting missing reviews\n",
    "missing_reviews = df_1[df_1.Reviews.isna()]\n",
    "urls = missing_reviews['url']\n",
    "len(list(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:22:09.985401Z",
     "start_time": "2022-07-26T17:22:09.894407Z"
    }
   },
   "outputs": [],
   "source": [
    "# Re-running function from above with the list of urls missing reviews\n",
    "if call_apis == True:\n",
    "    adding_reviews = get_text(list(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:22:11.200002Z",
     "start_time": "2022-07-26T17:22:11.150526Z"
    }
   },
   "outputs": [],
   "source": [
    "# Saving the review text as a csv to the repository\n",
    "if call_apis == True:\n",
    "    mssing_rvws = pd.DataFrame(adding_reviews,columns=['Review_Text'])\n",
    "    mssing_rvws.to_csv('mssing_rvws.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:22:15.581782Z",
     "start_time": "2022-07-26T17:22:15.265222Z"
    }
   },
   "outputs": [],
   "source": [
    "adding_reviews = pd.read_csv('mssing_rvws.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:22:16.739447Z",
     "start_time": "2022-07-26T17:22:16.679753Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_reviews2 = missing_reviews.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:22:53.547804Z",
     "start_time": "2022-07-26T17:22:53.462419Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_reviews2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:05.489390Z",
     "start_time": "2022-07-26T17:23:05.461913Z"
    }
   },
   "outputs": [],
   "source": [
    "urls_df = pd.DataFrame(urls)\n",
    "urls_df.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:06.765316Z",
     "start_time": "2022-07-26T17:23:06.751669Z"
    }
   },
   "outputs": [],
   "source": [
    "url_reviews_df = pd.concat([urls_df,adding_reviews],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:07.555199Z",
     "start_time": "2022-07-26T17:23:07.538266Z"
    }
   },
   "outputs": [],
   "source": [
    "url_reviews_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:08.657100Z",
     "start_time": "2022-07-26T17:23:08.377404Z"
    }
   },
   "outputs": [],
   "source": [
    "rvw_df = pd.read_csv('mssing_rvws.csv')\n",
    "rvw_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:11.287941Z",
     "start_time": "2022-07-26T17:23:11.180612Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_in_missing_df = pd.concat([missing_reviews2,adding_reviews],axis=1)\n",
    "fill_in_missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:12.258095Z",
     "start_time": "2022-07-26T17:23:12.253882Z"
    }
   },
   "outputs": [],
   "source": [
    "# fill_in_missing_df.rename(columns={'reviews':'Reviews'},inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:13.229751Z",
     "start_time": "2022-07-26T17:23:13.221954Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_in_missing_df['Review_Text'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:13.807903Z",
     "start_time": "2022-07-26T17:23:13.793135Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_in_missing_df.drop(columns=['Reviews'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:14.466496Z",
     "start_time": "2022-07-26T17:23:14.440795Z"
    }
   },
   "outputs": [],
   "source": [
    "# Appending businesses with new reviews to full dataset\n",
    "df_2 = pd.concat([df_1,fill_in_missing_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:15.344263Z",
     "start_time": "2022-07-26T17:23:15.248563Z"
    }
   },
   "outputs": [],
   "source": [
    "# dropping old rows without reviews\n",
    "df_2.dropna(subset=['Reviews'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:15.995564Z",
     "start_time": "2022-07-26T17:23:15.985617Z"
    }
   },
   "outputs": [],
   "source": [
    "df_2['Review_Text'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:18.133923Z",
     "start_time": "2022-07-26T17:23:17.964187Z"
    }
   },
   "outputs": [],
   "source": [
    "df_2[df_2['Reviews'] == '']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still 220 businesses without reviews. Not too bad, but will try to extract these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:19.406945Z",
     "start_time": "2022-07-26T17:23:19.387147Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_rvws2 = df_2[df_2['Reviews'] == '']['url']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:21.191961Z",
     "start_time": "2022-07-26T17:23:21.155670Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_rvws2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:24.191956Z",
     "start_time": "2022-07-26T17:23:24.179961Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extracting the last reviews\n",
    "# adding_reviews2 = get_text(list(missing_rvws2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:25.263643Z",
     "start_time": "2022-07-26T17:23:25.258400Z"
    }
   },
   "outputs": [],
   "source": [
    "# # Saving the review text as a csv to the repository\n",
    "# mssing_rvws2 = pd.DataFrame(adding_reviews2,columns=['Review_Text'])\n",
    "# mssing_rvws2.to_csv('mssing_rvws2.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:26.025903Z",
     "start_time": "2022-07-26T17:23:25.995287Z"
    }
   },
   "outputs": [],
   "source": [
    "mssing_rvws2 = pd.read_csv('mssing_rvws2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:26.704942Z",
     "start_time": "2022-07-26T17:23:26.562619Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_in_mssing_rvws_df2 = pd.concat([missing_rvws2_df,mssing_rvws2,],axis=1,join='outer',ignore_index=True)\n",
    "fill_in_mssing_rvws_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:27.253863Z",
     "start_time": "2022-07-26T17:23:27.195841Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_in_missing_df2 = pd.concat([missing_reviews2,adding_reviews],axis=1)\n",
    "fill_in_missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:29.422054Z",
     "start_time": "2022-07-26T17:23:29.388193Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_reviews_2_df = df_2[df_2['Reviews'] == '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:30.341865Z",
     "start_time": "2022-07-26T17:23:30.256468Z"
    }
   },
   "outputs": [],
   "source": [
    "missing_reviews_2_df.reset_index(inplace=True)\n",
    "missing_reviews_2_df.drop(columns=['index'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:31.568577Z",
     "start_time": "2022-07-26T17:23:31.463189Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_in_missing_df2 = pd.concat([missing_reviews_2_df,mssing_rvws2],axis=1)\n",
    "fill_in_missing_df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:32.460274Z",
     "start_time": "2022-07-26T17:23:32.391228Z"
    }
   },
   "outputs": [],
   "source": [
    "fill_in_missing_df2.drop(columns=['Reviews'],inplace=True)\n",
    "fill_in_missing_df2.rename(columns={'Review_Text':'Reviews'},inplace=True)\n",
    "fill_in_missing_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:33.358036Z",
     "start_time": "2022-07-26T17:23:33.300428Z"
    }
   },
   "outputs": [],
   "source": [
    "# Appending businesses with newly gathered reviews to full dataset\n",
    "df_3 = pd.concat([df_2,fill_in_missing_df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:34.136855Z",
     "start_time": "2022-07-26T17:23:34.089361Z"
    }
   },
   "outputs": [],
   "source": [
    "df_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:34.829413Z",
     "start_time": "2022-07-26T17:23:34.798934Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping old rows without reviews\n",
    "df_3 = df_3[df_3.Reviews != '']\n",
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:36.667429Z",
     "start_time": "2022-07-26T17:23:36.620441Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for duplicated rows\n",
    "df_3[df_3.duplicated(keep='first')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:37.384436Z",
     "start_time": "2022-07-26T17:23:37.353433Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping Duplicated Rows\n",
    "df_3.drop_duplicates(keep='first',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:38.150324Z",
     "start_time": "2022-07-26T17:23:38.119531Z"
    }
   },
   "outputs": [],
   "source": [
    "df_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:38.843106Z",
     "start_time": "2022-07-26T17:23:38.817045Z"
    }
   },
   "outputs": [],
   "source": [
    "# Resetting the index\n",
    "df_3.reset_index(inplace=True)\n",
    "df_3.drop(columns='index',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:39.455864Z",
     "start_time": "2022-07-26T17:23:39.427615Z"
    }
   },
   "outputs": [],
   "source": [
    "df_3.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:40.235476Z",
     "start_time": "2022-07-26T17:23:40.205368Z"
    }
   },
   "outputs": [],
   "source": [
    "# Looking at the distributions for each Boro\n",
    "df_3['BORO'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:41.000320Z",
     "start_time": "2022-07-26T17:23:40.965444Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inspecting missing Boro's\n",
    "df_3[df_3['BORO'] == '0' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:42.042166Z",
     "start_time": "2022-07-26T17:23:42.001389Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imputing missing BORO labels\n",
    "df_3.loc[6780:6781]['BORO'] = 'Brooklyn'\n",
    "df_3.loc[10182:10182]['BORO'] = 'Manhattan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:43.534761Z",
     "start_time": "2022-07-26T17:23:43.507814Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looking at Restaurants with missing zipcodes\n",
    "df_3[df_3['ZIPCODE'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:44.312678Z",
     "start_time": "2022-07-26T17:23:44.289223Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Looking at Restaurants with missing zipcodes\n",
    "missing_zips = df_3['ZIPCODE'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:45.052287Z",
     "start_time": "2022-07-26T17:23:45.023347Z"
    }
   },
   "outputs": [],
   "source": [
    "# Converting location data from str to dict dtype\n",
    "df_3['location'] = df_3['location'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:45.513980Z",
     "start_time": "2022-07-26T17:23:45.487567Z"
    }
   },
   "outputs": [],
   "source": [
    "# Confirming change\n",
    "type(df_3.iloc[0]['location'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:45.943320Z",
     "start_time": "2022-07-26T17:23:45.913660Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imputing Zipcodes missing from NYC data with Yelp value\n",
    "addrss_list = list(df_3['location'])\n",
    "zip_list =[]\n",
    "\n",
    "for zipcode in addrss_list:\n",
    "    zip_list.append(zipcode['zip_code'])\n",
    "    \n",
    "df_3['Zip_code'] = zip_list\n",
    "df_3.drop(columns=['ZIPCODE'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:46.396219Z",
     "start_time": "2022-07-26T17:23:46.364341Z"
    }
   },
   "outputs": [],
   "source": [
    "df_3['price'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:47.454462Z",
     "start_time": "2022-07-26T17:23:47.428875Z"
    }
   },
   "outputs": [],
   "source": [
    "# Converting all review text into string objects\n",
    "df_3['Reviews'].astype(str,errors='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring The Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:49.211725Z",
     "start_time": "2022-07-26T17:23:49.199681Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for bussiness still missing review data\n",
    "df_1['Reviews'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:50.387885Z",
     "start_time": "2022-07-26T17:23:50.358560Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping businesses missing reviews\n",
    "df_4 = df_1.dropna(subset=['Reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:51.948050Z",
     "start_time": "2022-07-26T17:23:51.935959Z"
    }
   },
   "outputs": [],
   "source": [
    "df_4['Reviews'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:54.168782Z",
     "start_time": "2022-07-26T17:23:53.596167Z"
    }
   },
   "outputs": [],
   "source": [
    "# Check for duplicates\n",
    "df_4.duplicated(keep='first').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:54.907769Z",
     "start_time": "2022-07-26T17:23:54.506417Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop duplicates\n",
    "df_4.drop_duplicates(keep='first', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:55.542472Z",
     "start_time": "2022-07-26T17:23:55.522209Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping unneccessary columns\n",
    "eda_df = df_4.drop(columns=['display_phone','phone','location','coordinates',\n",
    "                 'categories','is_closed','url','image_url','name','alias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:56.531160Z",
     "start_time": "2022-07-26T17:23:56.494248Z"
    }
   },
   "outputs": [],
   "source": [
    "eda_df['price'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:57.197225Z",
     "start_time": "2022-07-26T17:23:57.165598Z"
    }
   },
   "outputs": [],
   "source": [
    "eda_df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:23:59.835066Z",
     "start_time": "2022-07-26T17:23:59.132905Z"
    }
   },
   "outputs": [],
   "source": [
    "#Get the distribution of the ratings\n",
    "x=eda_df['rating'].value_counts()\n",
    "x=x.sort_index()\n",
    "#plot\n",
    "plt.figure(figsize=(8,4))\n",
    "ax= sns.barplot(x.index, x.values, alpha=0.8)\n",
    "plt.title(\"Star Rating Distribution\")\n",
    "plt.ylabel('# of businesses', fontsize=12)\n",
    "plt.xlabel('Star Ratings ', fontsize=12)\n",
    "\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:00.085554Z",
     "start_time": "2022-07-26T17:24:00.074432Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:02.111154Z",
     "start_time": "2022-07-26T17:24:02.077936Z"
    }
   },
   "outputs": [],
   "source": [
    "eda_df['rating'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:03.593590Z",
     "start_time": "2022-07-26T17:24:02.965986Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(eda_df.rating, kde=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:03.764694Z",
     "start_time": "2022-07-26T17:24:03.744804Z"
    }
   },
   "outputs": [],
   "source": [
    "cuisines = pd.DataFrame(eda_df['CUISINE DESCRIPTION'].value_counts())\n",
    "cuisines.reset_index(inplace=True)\n",
    "cuisines[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:05.782618Z",
     "start_time": "2022-07-26T17:24:04.666242Z"
    }
   },
   "outputs": [],
   "source": [
    "# What are the popular Cuisine categories?\n",
    "cuisine_cats = eda_df['CUISINE DESCRIPTION']\n",
    "\n",
    "x = cuisine_cats.value_counts()\n",
    "\n",
    "print(\"There are \",len(x),\" different types of cuisines in NYC\")\n",
    "\n",
    "#prep for chart\n",
    "x = x.sort_values(ascending=False)\n",
    "x = x.iloc[0:20]\n",
    "\n",
    "#chart\n",
    "plt.figure(figsize=(16,4))\n",
    "ax = sns.barplot(x.index, x.values, alpha=0.8)#,color=color[5])\n",
    "plt.title(\"What are the top categories?\",fontsize=25)\n",
    "locs, labels = plt.xticks()\n",
    "plt.setp(labels, rotation=80)\n",
    "plt.ylabel('# businesses', fontsize=12)\n",
    "plt.xlabel('Category', fontsize=12)\n",
    "\n",
    "#adding the text labels\n",
    "rects = ax.patches\n",
    "labels = x.values\n",
    "for rect, label in zip(rects, labels):\n",
    "    height = rect.get_height()\n",
    "    ax.text(rect.get_x() + rect.get_width()/2, height + 5, label, ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:05.799315Z",
     "start_time": "2022-07-26T17:24:05.787917Z"
    }
   },
   "outputs": [],
   "source": [
    "eda_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:06.285562Z",
     "start_time": "2022-07-26T17:24:06.250997Z"
    }
   },
   "outputs": [],
   "source": [
    "# Most reviewed restaurants\n",
    "eda_df[['CAMIS','id','DBA', 'review_count', 'rating']].sort_values(ascending=False, by=\"review_count\")[0:50]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:07.070357Z",
     "start_time": "2022-07-26T17:24:07.064998Z"
    }
   },
   "outputs": [],
   "source": [
    "# Look at all the feautres compared to the 0 class and 1 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:07.975737Z",
     "start_time": "2022-07-26T17:24:07.967988Z"
    }
   },
   "outputs": [],
   "source": [
    "labels_df = eda_df['Severe']\n",
    "labels_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:09.036651Z",
     "start_time": "2022-07-26T17:24:08.734419Z"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(9.2, 5))\n",
    "\n",
    "n_obs = labels_df.shape\n",
    "\n",
    "(eda_df['Severe']\n",
    "    .value_counts()\n",
    "    .plot.barh(title=\"Proportion of Restaurants with Severe Violations\", ax=ax)\n",
    ")\n",
    "ax.set_ylabel(\"Severe Violations\")\n",
    "\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:09.597133Z",
     "start_time": "2022-07-26T17:24:09.545265Z"
    }
   },
   "outputs": [],
   "source": [
    "counts = (eda_df[['BORO', 'Severe']]\n",
    "              .groupby(['BORO', 'Severe'])\n",
    "              .size()\n",
    "              .unstack('Severe')\n",
    "         )\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:11.563965Z",
     "start_time": "2022-07-26T17:24:11.055158Z"
    }
   },
   "outputs": [],
   "source": [
    "ax = counts.plot.barh()\n",
    "ax.legend(\n",
    "    loc='center right', \n",
    "    bbox_to_anchor=(1.3, 0.5), \n",
    "    title='Severe Violations'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:11.860808Z",
     "start_time": "2022-07-26T17:24:11.852002Z"
    }
   },
   "outputs": [],
   "source": [
    "severe_counts = counts.sum(axis='columns')\n",
    "severe_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:12.668952Z",
     "start_time": "2022-07-26T17:24:12.652142Z"
    }
   },
   "outputs": [],
   "source": [
    "props = counts.div(severe_counts, axis='index')\n",
    "props"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:13.679434Z",
     "start_time": "2022-07-26T17:24:13.313199Z"
    }
   },
   "outputs": [],
   "source": [
    "# Prototyping Stack Barh plot\n",
    "ax = props.plot.barh(stacked=True)\n",
    "ax.set_title('Severe Violations by Boro')\n",
    "ax.set_xlabel('Proportion of Restaurants w/Severe Violations ')\n",
    "ax.legend(\n",
    "    loc='center left', \n",
    "    bbox_to_anchor=(1.05, 0.5),\n",
    "    title='Severe Violations'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:13.997130Z",
     "start_time": "2022-07-26T17:24:13.988251Z"
    }
   },
   "outputs": [],
   "source": [
    "def violation_rate_plot(col, target, data, ax=None):\n",
    "    \"\"\"Stacked bar chart of severe health and safety violations rate against \n",
    "    each feature of the data. \n",
    "    \n",
    "    Args:\n",
    "        col (string): column name of feature variable\n",
    "        target (string): column name of target variable\n",
    "        data (pandas DataFrame): dataframe that contains columns \n",
    "            `col` and `target`\n",
    "        ax (matplotlib axes object, optional): matplotlib axes \n",
    "            object to attach plot to\n",
    "    \"\"\"\n",
    "    counts = (eda_df[[target, col]]\n",
    "                  .groupby([target, col])\n",
    "                  .size()\n",
    "                  .unstack(target)\n",
    "             )\n",
    "    group_counts = counts.sum(axis='columns')\n",
    "    props = counts.div(group_counts, axis='index')\n",
    "\n",
    "    props.plot(kind=\"barh\", stacked=True, ax=ax)\n",
    "\n",
    "    ax.legend().remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:15.366970Z",
     "start_time": "2022-07-26T17:24:15.360066Z"
    }
   },
   "outputs": [],
   "source": [
    "eda_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:17.263069Z",
     "start_time": "2022-07-26T17:24:15.491374Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loop through several columns and plot against both severe violations.\n",
    "\n",
    "cols_to_plot = [\n",
    "    'rating',\n",
    "    'transactions',\n",
    "    'price'\n",
    "]\n",
    "\n",
    "fig, ax = plt.subplots(\n",
    "    len(cols_to_plot), figsize=(10,len(cols_to_plot)*2.5)\n",
    ")\n",
    "for idx, col in enumerate(cols_to_plot):\n",
    "    \n",
    "    violation_rate_plot(\n",
    "        col, 'Severe', eda_df, ax=ax[idx]\n",
    "    )\n",
    "    \n",
    "\n",
    "ax[0].legend(\n",
    "    loc='lower center', bbox_to_anchor=(0.5, 1.05), title='Severe Violations'\n",
    ")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:17.289412Z",
     "start_time": "2022-07-26T17:24:16.265Z"
    }
   },
   "outputs": [],
   "source": [
    "# import street map\n",
    "# https://data.cityofnewyork.us/City-Government/Borough-Boundaries/tqmj-j8zm\n",
    "\n",
    "street_map = gpd.read_file('data/Borough_Boundaries/geo_export_392103e7-13e2-43bf-aaf0-7e5c6a24b7b2.shp')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:17.426958Z",
     "start_time": "2022-07-26T17:24:17.418172Z"
    }
   },
   "outputs": [],
   "source": [
    "geo_eda = eda_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:18.242870Z",
     "start_time": "2022-07-26T17:24:18.227072Z"
    }
   },
   "outputs": [],
   "source": [
    "geo_eda.dropna(axis=0, subset=['Latitude','Longitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:18.950489Z",
     "start_time": "2022-07-26T17:24:18.923427Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove the most extreme .1% latitudes, &\n",
    "# the most extreme .1% longitudes\n",
    "\n",
    "geo_eda = geo_eda[\n",
    " (geo_eda['Latitude'] >= np.percentile(geo_eda['Latitude'], 0.05)) & \n",
    " (geo_eda['Latitude'] < np.percentile(geo_eda['Latitude'], 99.95)) &\n",
    " (geo_eda['Longitude'] >= np.percentile(geo_eda['Longitude'], 0.05)) & \n",
    " (geo_eda['Longitude'] <= np.percentile(geo_eda['Longitude'], 99.95))\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:19.706799Z",
     "start_time": "2022-07-26T17:24:19.623266Z"
    }
   },
   "outputs": [],
   "source": [
    "geo_eda = geo_eda[geo_eda['Longitude'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:20.226927Z",
     "start_time": "2022-07-26T17:24:20.216188Z"
    }
   },
   "outputs": [],
   "source": [
    "geo_eda = geo_eda[geo_eda['Latitude'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:20.880138Z",
     "start_time": "2022-07-26T17:24:20.870424Z"
    }
   },
   "outputs": [],
   "source": [
    "print(geo_eda['Longitude'].max())\n",
    "print(geo_eda['Longitude'].min())\n",
    "print(geo_eda['Latitude'].max())\n",
    "print(geo_eda['Latitude'].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:21.724012Z",
     "start_time": "2022-07-26T17:24:21.407779Z"
    }
   },
   "outputs": [],
   "source": [
    "# designate coordinate system\n",
    "crs = {'init':'epsg:4326'}\n",
    "# zip x and y coordinates into single feature\n",
    "geometry = [Point(xy) for xy in zip(geo_eda['Longitude'], geo_eda['Latitude'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:22.749699Z",
     "start_time": "2022-07-26T17:24:21.875758Z"
    }
   },
   "outputs": [],
   "source": [
    "# create GeoPandas dataframe\n",
    "geo_df = gpd.GeoDataFrame(geo_eda, crs=crs, geometry = geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:23.111440Z",
     "start_time": "2022-07-26T17:24:22.758424Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plotting map\n",
    "fig, ax = plt.subplots(figsize=(15,15))\n",
    "street_map.plot(ax=ax, alpha=.5,color='gray')\n",
    "geo_df[geo_df['Severe'] == 0].plot(ax=ax, alpha=.1, markersize=20,color='blue',label='Pass')\n",
    "geo_df[geo_df['Severe'] == 1].plot(ax=ax, alpha=.1, markersize=20,color='red',label='Fail')\n",
    "plt.legend(prop={'size':15});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing For Further EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets find most frequent words in Negative reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically, we will find most frequent words in reviews to get an overview of why users gave low ratings. These words could be related to those business attributes or services about which users are most unhappy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:24.825296Z",
     "start_time": "2022-07-26T17:24:24.809495Z"
    }
   },
   "outputs": [],
   "source": [
    "txt_eda = df_4.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:25.585004Z",
     "start_time": "2022-07-26T17:24:25.527184Z"
    }
   },
   "outputs": [],
   "source": [
    "passed_df = txt_eda.loc[txt_eda['Severe']==0]\n",
    "passed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:29.419594Z",
     "start_time": "2022-07-26T17:24:26.462228Z"
    }
   },
   "outputs": [],
   "source": [
    "failed_df = txt_eda.loc[txt_eda['Severe']==1]\n",
    "failed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:29.465244Z",
     "start_time": "2022-07-26T17:24:29.433134Z"
    }
   },
   "outputs": [],
   "source": [
    "passed_corpus = passed_df['Reviews'].to_list()\n",
    "passed_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:24:29.487408Z",
     "start_time": "2022-07-26T17:24:29.475842Z"
    }
   },
   "outputs": [],
   "source": [
    "failed_corpus = failed_df['Reviews'].to_list()\n",
    "failed_corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:25:40.921145Z",
     "start_time": "2022-07-26T17:24:29.494874Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizing the corpus of review text from restaurants that have severly failed their inspection\n",
    "failed_tokens = word_tokenize(','.join(failed_corpus))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:28:24.897185Z",
     "start_time": "2022-07-26T17:25:40.924524Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizing the corpus of review text from restaurants that have not severly  their inspection\n",
    "passed_tokens = word_tokenize(','.join(passed_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:28:24.932368Z",
     "start_time": "2022-07-26T17:28:24.904881Z"
    }
   },
   "outputs": [],
   "source": [
    "# Importing English stop words\n",
    "stopwords_list = stopwords.words('english')\n",
    "# Add punctuation marks to the stopwords_list\n",
    "stopwords_list.extend(string.punctuation)\n",
    "additional_punc = ['“','”','...',\"''\",'’','``']\n",
    "stopwords_list.extend(additional_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:28:48.485212Z",
     "start_time": "2022-07-26T17:28:24.941851Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing stopwords from failed_corpus\n",
    "stopped_failed_tokens = [w.lower() for w in failed_tokens if w.lower() not in stopwords_list]\n",
    "stopped_failed_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:29:40.694205Z",
     "start_time": "2022-07-26T17:28:48.536397Z"
    }
   },
   "outputs": [],
   "source": [
    "# Removing stopwords from passed_corpus\n",
    "stopped_passed_tokens = [w.lower() for w in passed_tokens if w.lower() not in stopwords_list]\n",
    "stopped_passed_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:29:44.817645Z",
     "start_time": "2022-07-26T17:29:40.699563Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating FreqDist from stopped_failed_tokens\n",
    "freq_failed = FreqDist(stopped_failed_tokens)\n",
    "freq_failed.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:29:53.099059Z",
     "start_time": "2022-07-26T17:29:44.822725Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating FreqDist from stopped_passed_tokens\n",
    "freq_passed = FreqDist(stopped_passed_tokens)\n",
    "freq_passed.most_common(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:29:53.123715Z",
     "start_time": "2022-07-26T17:29:53.116837Z"
    }
   },
   "outputs": [],
   "source": [
    "# Functionizing wordcloud generator\n",
    "def wordcloud_generator(tokens, collocations=False, background_color='black', \n",
    "                       colormap='Reds', display=True):\n",
    "\n",
    "    \n",
    "    # Initalize a WordCloud\n",
    "    wordcloud = WordCloud(collocations=collocations, \n",
    "                          background_color=background_color, \n",
    "                          colormap=colormap, \n",
    "                          width=500, height=300)\n",
    "\n",
    "    # Generate wordcloud from tokens\n",
    "    wordcloud.generate(','.join(tokens))\n",
    "\n",
    "    # Plot with matplotlib\n",
    "    if display:\n",
    "        plt.figure(figsize = (12, 15), facecolor = None) \n",
    "        plt.imshow(wordcloud) \n",
    "        plt.axis('off');\n",
    "        \n",
    "    return wordcloud\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:30:28.592816Z",
     "start_time": "2022-07-26T17:29:53.131543Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generating a WordCloud for reviews from restaurants that have failed their inspection\n",
    "failed_cloud = wordcloud_generator(stopped_failed_tokens, collocations=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:31:25.878862Z",
     "start_time": "2022-07-26T17:30:28.630805Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate a WordCloud from reviews of restaurants that have not severely failed an inspection\n",
    "passsed_cloud = wordcloud_generator(stopped_passed_tokens, colormap='Greens',\n",
    "                                     collocations=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:31:50.780477Z",
     "start_time": "2022-07-26T17:31:25.887942Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bigrams from reviews of failed inspections\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "review_finder = nltk.BigramCollocationFinder.from_words(stopped_failed_tokens)\n",
    "reviews_scored = review_finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:31:52.099214Z",
     "start_time": "2022-07-26T17:31:50.784645Z"
    }
   },
   "outputs": [],
   "source": [
    "# df from the Bigrams\n",
    "pd.DataFrame(reviews_scored, columns=[\"Word\",\"Freq\"]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:32:40.272083Z",
     "start_time": "2022-07-26T17:31:52.111555Z"
    }
   },
   "outputs": [],
   "source": [
    "# Bigrams from reviews of restaurants passed their inspections\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "review_finder = nltk.BigramCollocationFinder.from_words(stopped_passed_tokens)\n",
    "reviews_scored = review_finder.score_ngrams(bigram_measures.raw_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:32:42.660599Z",
     "start_time": "2022-07-26T17:32:40.290828Z"
    }
   },
   "outputs": [],
   "source": [
    "# df from the Bigrams\n",
    "pd.DataFrame(reviews_scored, columns=[\"Word\",\"Freq\"]).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:32:42.807923Z",
     "start_time": "2022-07-26T17:32:42.763062Z"
    }
   },
   "outputs": [],
   "source": [
    "# FREQ DIST PLOT FOR MOST FREQ WORDS IN PASSED AND FAILED RESTAURANTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:32:47.051820Z",
     "start_time": "2022-07-26T17:32:42.814119Z"
    }
   },
   "outputs": [],
   "source": [
    "fdist = FreqDist(stopped_failed_tokens)\n",
    "plt.figure(figsize=(10, 10))\n",
    "fdist.plot(30);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:32:47.937963Z",
     "start_time": "2022-07-26T17:32:47.075751Z"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:32:48.030758Z",
     "start_time": "2022-07-26T17:32:47.943669Z"
    }
   },
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:32:48.045124Z",
     "start_time": "2022-07-26T17:32:48.040087Z"
    }
   },
   "outputs": [],
   "source": [
    "a =  'This is an excellent restaurant!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:32:48.084857Z",
     "start_time": "2022-07-26T17:32:48.052509Z"
    }
   },
   "outputs": [],
   "source": [
    "sid.polarity_scores(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:32:48.519941Z",
     "start_time": "2022-07-26T17:32:48.091182Z"
    }
   },
   "outputs": [],
   "source": [
    "df_5['price'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:24.449678Z",
     "start_time": "2022-07-26T17:33:24.402392Z"
    }
   },
   "outputs": [],
   "source": [
    "# Mapping prices to ordinal categories\n",
    "price_dict={'$$$$': 4, '$$$': 3, '$$': 2, '$': 1}\n",
    "\n",
    "df_5['prices'] = df_5['price'].map(price_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:25.865078Z",
     "start_time": "2022-07-26T17:33:25.824750Z"
    }
   },
   "outputs": [],
   "source": [
    "# Imputing a zeros for NAN values\n",
    "df_5['prices'].fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:26.696532Z",
     "start_time": "2022-07-26T17:33:26.670932Z"
    }
   },
   "outputs": [],
   "source": [
    "transactions = df_5['transactions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:28.735890Z",
     "start_time": "2022-07-26T17:33:28.698513Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use ast module to convert string objects into list\n",
    "transactions = transactions.apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:29.483753Z",
     "start_time": "2022-07-26T17:33:29.454026Z"
    }
   },
   "outputs": [],
   "source": [
    "# Change list into dicts\n",
    "dcts = transactions.apply(lambda x: {c: 1 for c in x})\n",
    "\n",
    "# Create new dataframe based on the list of dictionaries.\n",
    "ohe_df = pd.DataFrame(dcts.tolist()).fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:31.307427Z",
     "start_time": "2022-07-26T17:33:31.277561Z"
    }
   },
   "outputs": [],
   "source": [
    "ohe_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:32.055935Z",
     "start_time": "2022-07-26T17:33:32.031168Z"
    }
   },
   "outputs": [],
   "source": [
    "# Concatenate dummy variales with the full dataset\n",
    "df_5 = pd.concat([df_5,ohe_df],axis=1)\n",
    "df_5.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:33.199120Z",
     "start_time": "2022-07-26T17:33:33.162899Z"
    }
   },
   "outputs": [],
   "source": [
    "df_5.drop(columns=['price','transactions'],inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing For Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:38.380337Z",
     "start_time": "2022-07-26T17:33:38.372181Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_4.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:39.879961Z",
     "start_time": "2022-07-26T17:33:39.360436Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping unneccessary columns\n",
    "model_df = df_4.drop(columns=['CAMIS', 'DBA', 'CUISINE DESCRIPTION', 'BORO',\n",
    "                               'BUILDING', 'STREET','ZIPCODE', 'PHONE',\n",
    "                               'Latitude', 'Longitude', 'Community Board',\n",
    "                               'Council District', 'Census Tract', \n",
    "                               'id', 'alias', 'name','image_url', 'is_closed',\n",
    "                               'url', 'review_count', 'categories', 'rating',\n",
    "                               'coordinates', 'transactions', 'price',\n",
    "                               'location', 'phone','display_phone'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:41.196213Z",
     "start_time": "2022-07-26T17:33:41.167497Z"
    }
   },
   "outputs": [],
   "source": [
    "model_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:44.118078Z",
     "start_time": "2022-07-26T17:33:44.102743Z"
    }
   },
   "outputs": [],
   "source": [
    "model_df['Reviews'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:46.669502Z",
     "start_time": "2022-07-26T17:33:45.318875Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking for duplicates\n",
    "model_df.duplicated(keep='first').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:47.393606Z",
     "start_time": "2022-07-26T17:33:46.933455Z"
    }
   },
   "outputs": [],
   "source": [
    "# Dropping duplicates\n",
    "model_df.drop_duplicates(keep='first', inplace=True)\n",
    "\n",
    "# Confirming duplicates have been dropped\n",
    "\n",
    "model_df.duplicated(keep='first').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:48.350733Z",
     "start_time": "2022-07-26T17:33:48.322616Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking Class Balance\n",
    "model_df['Severe'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:49.299625Z",
     "start_time": "2022-07-26T17:33:49.253910Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make X and y\n",
    "y = model_df['Severe'].copy()\n",
    "X = model_df['Reviews'].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:50.162591Z",
     "start_time": "2022-07-26T17:33:50.052988Z"
    }
   },
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:50.832208Z",
     "start_time": "2022-07-26T17:33:50.810873Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using NLTK's Regular Expressions Tokenizer from nltk.tokenize\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:33:51.420304Z",
     "start_time": "2022-07-26T17:33:51.406624Z"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a Count Vectorizer the RE tokenizer's .tokenize method\n",
    "vectorizer = CountVectorizer(lowercase=True, tokenizer=tokenizer.tokenize,\n",
    "                            stop_words=stopwords_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:34:10.686624Z",
     "start_time": "2022-07-26T17:33:53.471594Z"
    }
   },
   "outputs": [],
   "source": [
    "# Vectorizing the data and saving X_train_bow and X_test_bow\n",
    "X_train_bow = vectorizer.fit_transform(X_train)\n",
    "X_test_bow = vectorizer.transform(X_test)\n",
    "X_train_bow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag-of-Words Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:34:16.429440Z",
     "start_time": "2022-07-26T17:34:10.726070Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create and fit a Decision Tree Classifier\n",
    "dt = DecisionTreeClassifier(class_weight='balanced', max_depth=10)\n",
    "dt.fit(X_train_bow,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:34:16.465756Z",
     "start_time": "2022-07-26T17:34:16.436160Z"
    }
   },
   "outputs": [],
   "source": [
    "# Extract predictions for the train and test sets\n",
    "y_hat_test = dt.predict(X_test_bow)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:34:24.887739Z",
     "start_time": "2022-07-26T17:34:24.862941Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_model(y_test,y_hat_test,X_test,clf=None,\n",
    "                  scoring=metrics.recall_score,verbose=False,\n",
    "                  scorer=False,classes=['Passed','Failed']):\n",
    "    \"\"\"Quick/simple classification model evaluation\"\"\"\n",
    "\n",
    "    print(metrics.classification_report(y_test,y_hat_test,\n",
    "                                        target_names=classes))\n",
    "    \n",
    "    metrics.plot_confusion_matrix(clf,X_test,y_test,normalize='true',\n",
    "                                 cmap='Blues',display_labels=classes)\n",
    "    plt.show()\n",
    "    if verbose:\n",
    "        print(\"MODEL PARAMETERS:\")\n",
    "        print(pd.Series(dt.get_params()))\n",
    "        \n",
    "    if scorer:\n",
    "        \n",
    "        return scoring(y_test,y_hat_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:34:26.532837Z",
     "start_time": "2022-07-26T17:34:26.046453Z"
    }
   },
   "outputs": [],
   "source": [
    "# Evaluating the moodel using function\n",
    "evaluate_model(y_test,y_hat_test,X_test_bow,dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:34:27.720062Z",
     "start_time": "2022-07-26T17:34:26.708295Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the top 30 most important features\n",
    "with plt.style.context('seaborn-talk'):\n",
    "\n",
    "# Get Feature Importance\n",
    "    importance = pd.Series(dt.feature_importances_,index=vectorizer.get_feature_names())\n",
    "\n",
    "# Sort values \n",
    "\n",
    "\n",
    "# Take the .tail 30 and plot kind='barh'\n",
    "    importance.sort_values().tail(30).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:34:27.995037Z",
     "start_time": "2022-07-26T17:34:27.901758Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "tf_transform = TfidfTransformer(use_idf=True)\n",
    "\n",
    "text_pipe = Pipeline(steps=[\n",
    "    ('count_vectorizer',count_vect),\n",
    "    ('tf_transformer',tf_transform)])\n",
    "\n",
    "full_pipe = Pipeline(steps=[\n",
    "    ('text_pipe',text_pipe),\n",
    "    ('clf',DecisionTreeClassifier(class_weight='balanced'))\n",
    "])\n",
    "full_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:34:49.869843Z",
     "start_time": "2022-07-26T17:34:29.109391Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "## Preview current X_train\n",
    "X_train_pipe = text_pipe.fit_transform(X_train)\n",
    "X_test_pipe = text_pipe.transform(X_test)\n",
    "X_train_pipe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:34:50.000703Z",
     "start_time": "2022-07-26T17:34:49.930522Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn import set_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:34:50.132736Z",
     "start_time": "2022-07-26T17:34:50.007783Z"
    }
   },
   "outputs": [],
   "source": [
    "set_config(display='diagram')\n",
    "\n",
    "full_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:55:17.521314Z",
     "start_time": "2022-07-26T17:34:50.140716Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "## Make a tokenizer with TweetTokenizer\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+')\n",
    "vectorizer = CountVectorizer(ngram_range=[1,2])\n",
    "## Make params Grid\n",
    "#### use_idf: True/False\n",
    "#### tokenizer: None, tokenizer.tokenize\n",
    "#### criterion: gini, entropy\n",
    "#### stopwords\n",
    "\n",
    "params = {'text_pipe__tf_transformer__use_idf':[True, False],\n",
    "         'text_pipe__count_vectorizer__tokenizer':[None,tokenizer.tokenize],\n",
    "         'text_pipe__count_vectorizer__stop_words':[None,stopwords_list],\n",
    "         'clf__criterion':['gini', 'entropy']}\n",
    "\n",
    "## Make and fit grid\n",
    "grid = GridSearchCV(full_pipe,params,cv=3)\n",
    "grid.fit(X_train,y_train)\n",
    "## Display best params\n",
    "grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:55:17.576702Z",
     "start_time": "2022-07-26T17:55:17.526866Z"
    }
   },
   "outputs": [],
   "source": [
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\",\n",
    "                      stop_words=sw, ngram_range=[1,2],\n",
    "                      min_df=2, max_df=25)\n",
    "X = vec.fit_transform(corpus.body)\n",
    "\n",
    "df_cv = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:55:39.613117Z",
     "start_time": "2022-07-26T17:55:36.644384Z"
    }
   },
   "outputs": [],
   "source": [
    "## Evluate the best_estimator\n",
    "best_pipe = grid.best_estimator_\n",
    "y_hat_test = grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:55:44.861331Z",
     "start_time": "2022-07-26T17:55:40.714860Z"
    }
   },
   "outputs": [],
   "source": [
    "evaluate_model(y_test,y_hat_test,X_test,best_pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting Feature Importances\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:55:56.330958Z",
     "start_time": "2022-07-26T17:55:44.867332Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pipe = text_pipe.fit_transform(X_train)\n",
    "X_test_pipe = text_pipe.transform(X_test)\n",
    "X_train_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:55:56.348426Z",
     "start_time": "2022-07-26T17:55:56.336490Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train_pipe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:55:56.454480Z",
     "start_time": "2022-07-26T17:55:56.354175Z"
    }
   },
   "outputs": [],
   "source": [
    "features = text_pipe.named_steps['count_vectorizer'].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:55:56.469396Z",
     "start_time": "2022-07-26T17:55:56.459497Z"
    }
   },
   "outputs": [],
   "source": [
    "len(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:55:56.549317Z",
     "start_time": "2022-07-26T17:55:56.472791Z"
    }
   },
   "outputs": [],
   "source": [
    "# vectorizer.get_feature_names()\n",
    "rf = best_pipe.named_steps['clf']\n",
    "with plt.style.context('seaborn-talk'):\n",
    "    importance = pd.Series(rf.feature_importances_,index=features)#vectorizer.get_feature_names())\n",
    "    importance.sort_values(inplace=True)\n",
    "\n",
    "    importance.sort_values().tail(30).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:58:57.936030Z",
     "start_time": "2022-07-26T17:58:57.893870Z"
    }
   },
   "outputs": [],
   "source": [
    "top_word_probs = {}\n",
    "for word in importance.tail(20).index:\n",
    "    rows = df['text'].str.contains(word,regex=False,case=False)\n",
    "    val_count= df[rows]['source'].value_counts(normalize=True)\n",
    "    top_word_probs[word] = val_count\n",
    "#     print(f'\\n\\n{word}:\\n{val_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:55:56.558453Z",
     "start_time": "2022-07-26T17:55:51.727Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "top_probs = pd.DataFrame(top_word_probs).T\n",
    "top_probs.style.background_gradient(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:55:56.845812Z",
     "start_time": "2022-07-26T17:55:56.812515Z"
    }
   },
   "outputs": [],
   "source": [
    "### Function to produce the model's coefficients\n",
    "\n",
    "def eval_clf(model, X_test_tf,y_test,cmap='Reds',\n",
    "                            normalize='true',classes=['Unvaccinated', 'Vaccinated'],figsize=(10,4),\n",
    "                            X_train = None, y_train = None,):\n",
    "    \"\"\"Evaluates a scikit-learn binary classification model.\n",
    "\n",
    "    Args:\n",
    "        model ([type]): [description]\n",
    "        X_test_tf ([type]): [description]\n",
    "        y_test ([type]): [description]\n",
    "        cmap (str, optional): [description]. Defaults to 'Reds'.\n",
    "        normalize (str, optional): [description]. Defaults to 'true'.\n",
    "        classes ([type], optional): [description]. Defaults to None.\n",
    "        figsize (tuple, optional): [description]. Defaults to (8,4).\n",
    "        X_train ([type], optional): [description]. Defaults to None.\n",
    "        y_train ([type], optional): [description]. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    y_hat_test = model.predict(X_test_tf)\n",
    "    print(metrics.classification_report(y_test, y_hat_test,target_names=classes))\n",
    "    \n",
    "\n",
    "    fig,ax = plt.subplots(ncols=2,figsize=figsize)\n",
    "    plt.grid(False)\n",
    "    plot_confusion_matrix(model, X_test_tf,y_test,cmap=cmap, \n",
    "                                  normalize=normalize,display_labels=classes,\n",
    "                                 ax=ax[0])\n",
    "    for a in ax:\n",
    "        a.grid(False)   \n",
    "        \n",
    "    curve = metrics.plot_roc_curve(model,X_test_tf,y_test,ax=ax[1])\n",
    "    curve.ax_.grid()\n",
    "    curve.ax_.plot([0,1],[0,1],ls=':')\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    ## Add comparing Scores if X_train and y_train provided.\n",
    "    if (X_train is not None) & (y_train is not None):\n",
    "        print(f\"Training Score = {model.score(X_train,y_train):.2f}\")\n",
    "        print(f\"Test Score = {model.score(X_test_tf,y_test):.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:55:59.130816Z",
     "start_time": "2022-07-26T17:55:59.104411Z"
    }
   },
   "outputs": [],
   "source": [
    "## Creating baseline classifier model\n",
    "\n",
    "base = DummyClassifier(strategy='stratified', random_state = 42)\n",
    "\n",
    "base.fit(X_train_df, y_train)\n",
    "\n",
    "eval_clf(base,X_test_tf,y_test,X_train=X_train_df,y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:56:03.117277Z",
     "start_time": "2022-07-26T17:56:03.111083Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a Regular Expression Tokenizer from nltk.tokenize\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z0-9]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:56:03.664108Z",
     "start_time": "2022-07-26T17:56:03.630553Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a TfIdf Vectorizer using tweet tokenizer's .tokenize method\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenizer.tokenize,\n",
    "                             stop_words=stopwords_list)\n",
    "\n",
    "# Vectorize data and make X_train_tfidf and X_test_tfidf\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "X_train_tfidf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:56:06.373970Z",
     "start_time": "2022-07-26T17:56:06.348217Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_vec = TfidfVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=stopwords_list)\n",
    "X = tf_vec.fit_transform(X_train)\n",
    "\n",
    "df = pd.DataFrame(X.toarray(), columns = tf_vec.get_feature_names())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:56:07.184496Z",
     "start_time": "2022-07-26T17:56:07.155009Z"
    }
   },
   "outputs": [],
   "source": [
    "df.iloc[33].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:56:21.999885Z",
     "start_time": "2022-07-26T17:56:10.053311Z"
    }
   },
   "outputs": [],
   "source": [
    "# Comparing to the CountVectorizor\n",
    "vec = CountVectorizer(token_pattern=r\"([a-zA-Z]+(?:'[a-z]+)?)\", stop_words=stopwords_list)\n",
    "X = vec.fit_transform(X_train)\n",
    "\n",
    "df_cv = pd.DataFrame(X.toarray(), columns=vec.get_feature_names())\n",
    "df_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:56:22.042269Z",
     "start_time": "2022-07-26T17:56:22.009106Z"
    }
   },
   "outputs": [],
   "source": [
    "df_cv.iloc[33].sort_values(ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:56:22.070973Z",
     "start_time": "2022-07-26T17:56:22.047334Z"
    }
   },
   "outputs": [],
   "source": [
    "tf_vec.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-25T22:25:45.624914Z",
     "start_time": "2022-05-25T22:25:45.612791Z"
    }
   },
   "source": [
    "### Modeling Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:56:31.179884Z",
     "start_time": "2022-07-26T17:56:31.152450Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make and fit a decision tree  (class_weight='balanced')\n",
    "dt = DecisionTreeClassifier(max_depth=6, class_weight='balanced')\n",
    "dt.fit(X_train_tfidf,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:56:22.076394Z",
     "start_time": "2022-07-26T17:56:16.210Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_classification(model, X_test_tf,y_test,cmap='Greens',\n",
    "                            normalize='true',classes=None,figsize=(10,4),\n",
    "                            X_train = None, y_train = None,):\n",
    "    \"\"\"Evaluates a scikit-learn binary classification model.\n",
    "\n",
    "    Args:\n",
    "        model (classifier): any sklearn classification model.\n",
    "        X_test_tf (Frame or Array): X data\n",
    "        y_test (Series or Array): y data\n",
    "        cmap (str, optional): Colormap for confusion matrix. Defaults to 'Greens'.\n",
    "        normalize (str, optional): normalize argument for plot_confusion_matrix. \n",
    "                                    Defaults to 'true'.\n",
    "        classes (list, optional): List of class names for display. Defaults to None.\n",
    "        figsize (tuple, optional): figure size Defaults to (8,4).\n",
    "        \n",
    "        X_train (Frame or Array, optional): If provided, compare model.score \n",
    "                                for train and test. Defaults to None.\n",
    "        y_train (Series or Array, optional): If provided, compare model.score \n",
    "                                for train and test. Defaults to None.\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Get Predictions and Classification Report\n",
    "    y_hat_test = model.predict(X_test_tf)\n",
    "    print(metrics.classification_report(y_test, y_hat_test,target_names=classes))\n",
    "    \n",
    "    ## Plot Confusion Matrid and roc curve\n",
    "    fig,ax = plt.subplots(ncols=2, figsize=figsize)\n",
    "    metrics.plot_confusion_matrix(model, X_test_tf,y_test,cmap=cmap, \n",
    "                                  normalize=normalize,display_labels=classes,\n",
    "                                 ax=ax[0])\n",
    "    \n",
    "    ## if roc curve erorrs, delete second ax\n",
    "    try:\n",
    "        curve = metrics.plot_roc_curve(model,X_test_tf,y_test,ax=ax[1])\n",
    "        curve.ax_.grid()\n",
    "        curve.ax_.plot([0,1],[0,1],ls=':')\n",
    "        fig.tight_layout()\n",
    "    except:\n",
    "        fig.delaxes(ax[1])\n",
    "    plt.show()\n",
    "    \n",
    "    ## Add comparing Scores if X_train and y_train provided.\n",
    "    if (X_train is not None) & (y_train is not None):\n",
    "        print(f\"Training Score = {model.score(X_train,y_train):.2f}\")\n",
    "        print(f\"Test Score = {model.score(X_test_tf,y_test):.2f}\")\n",
    "        \n",
    "    \n",
    "def plot_importance(tree, X_train_df, top_n=20,figsize=(10,10)):\n",
    "    \n",
    "    df_importance = pd.Series(tree.feature_importances_,\n",
    "                              index=X_train_df.columns)\n",
    "    df_importance.sort_values(ascending=True).tail(top_n).plot(\n",
    "        kind='barh',figsize=figsize,title='Feature Importances',\n",
    "    ylabel='Feature',)\n",
    "    return df_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:56:22.084149Z",
     "start_time": "2022-07-26T17:56:16.688Z"
    }
   },
   "outputs": [],
   "source": [
    "## Evaluate Model using function\n",
    "evaluate_classification(dt,X_test_tfidf,y_test,X_train=X_train_tfidf,y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:56:22.086869Z",
     "start_time": "2022-07-26T17:56:17.320Z"
    }
   },
   "outputs": [],
   "source": [
    "# Plot the top 30 most important features\n",
    "with plt.style.context('seaborn-talk'):\n",
    "\n",
    "    ## Get Feature Importance\n",
    "    importance = pd.Series(dt.feature_importances_,\n",
    "                           index=vectorizer.get_feature_names())\n",
    "\n",
    "    ## Take the .tail 30 and plot kind='barh'\n",
    "    importance.sort_values().tail(30).plot(kind='barh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:56:22.089705Z",
     "start_time": "2022-07-26T17:56:17.958Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a text preprocessing pipeline\n",
    "text_pipe = Pipeline(steps=[\n",
    "    ('count_vectorizer',CountVectorizer()),\n",
    "    ('tf_transformer',TfidfTransformer(use_idf=True))\n",
    "])\n",
    "text_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:56:22.092638Z",
     "start_time": "2022-07-26T17:56:18.344Z"
    }
   },
   "outputs": [],
   "source": [
    "## Test out the text pipeline on X_train\n",
    "X_train_pipe = text_pipe.fit_transform(X_train)\n",
    "X_test_pipe = text_pipe.transform(X_test)\n",
    "X_train_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:56:22.103725Z",
     "start_time": "2022-07-26T17:56:18.750Z"
    }
   },
   "outputs": [],
   "source": [
    "## Make a full pipeline with the random forest model as the second step\n",
    "full_pipe = Pipeline([('text_pipe',text_pipe),\n",
    "                     ('clf',DecisionTreeClassifier(max_depth=6,class_weight='balanced'))])\n",
    "full_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-26T17:56:22.106431Z",
     "start_time": "2022-07-26T17:56:19.087Z"
    }
   },
   "outputs": [],
   "source": [
    "## Modeling with full pipeline\n",
    "full_pipe.fit(X_train,y_train)\n",
    "evaluate_classification(full_pipe,X_test,y_test,X_train=X_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T01:32:05.417989Z",
     "start_time": "2022-07-29T01:32:05.349005Z"
    }
   },
   "outputs": [],
   "source": [
    "# DTREE Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T01:32:12.027494Z",
     "start_time": "2022-07-29T01:32:12.017691Z"
    }
   },
   "outputs": [],
   "source": [
    "# SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  RNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T01:33:13.626989Z",
     "start_time": "2022-07-29T01:33:13.620447Z"
    }
   },
   "outputs": [],
   "source": [
    "# All lowercase\n",
    "# No stopword removal\n",
    "# No stemming/lemma"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Deep NLP Models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T01:40:19.758328Z",
     "start_time": "2022-07-29T01:40:08.017229Z"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TO-DO\n",
    "# Deep NLP\n",
    "# Train on word2vec /glove model\n",
    "# LSTM or GRU layers\n",
    "# Sequential – keras models \n",
    "# Blackbox models (lime)\n",
    "# #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpreting LIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T01:55:03.617225Z",
     "start_time": "2022-07-29T01:55:03.579344Z"
    }
   },
   "outputs": [],
   "source": [
    "# https://www.kdnuggets.com/2022/01/explain-nlp-models-lime.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-07-29T01:54:10.810810Z",
     "start_time": "2022-07-29T01:54:08.866690Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lime'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-07464ae56979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# for LIME import necessary packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlime\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlime_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlime_text\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLimeTextExplainer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpipeline\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlime_text\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mIndexedString\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIndexedCharacters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lime'"
     ]
    }
   ],
   "source": [
    "# for LIME import necessary packages\n",
    "from lime import lime_text\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from lime.lime_text import IndexedString,IndexedCharacters\n",
    "from lime.lime_base import LimeBase\n",
    "from sklearn.linear_model import Ridge, lars_path\n",
    "from lime.lime_text import explanation\n",
    "from functools import partial\n",
    "import scipy as sp\n",
    "from sklearn.utils import check_random_state\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpreting Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the names of the features\n",
    "feature_names = np.array(vec.get_feature_names())\n",
    "\n",
    "def get_top_features(features, model, level, limit, bottom=False):\n",
    "    \"\"\" Get the top (most likely to see violations) and bottom (least\n",
    "        likely to see violations) features for a given model.\n",
    "        \n",
    "        :param features: an array of the feature names\n",
    "        :param model: a fitted linear regression model\n",
    "        :param level: 0, 1, 2 for *, **, *** violation levels\n",
    "        :param limit: how many features to return\n",
    "        :param worst: if we want the bottom features rather than the top \n",
    "    \"\"\"\n",
    "    # sort order for the coefficients\n",
    "    sorted_coeffs = np.argsort(model.coef_[i])\n",
    "    \n",
    "    if bottom:\n",
    "        # get the features at the end of the sorted list\n",
    "        return features[sorted_coeffs[-1 * limit:]]\n",
    "    else:\n",
    "        # get the features at the beginning of the sorted list\n",
    "        return features[sorted_coeffs[:limit]]\n",
    "    \n",
    "# get the features that indicate we are most and least likely to see violations\n",
    "worst_feature_sets = [get_top_features(feature_names, ols, i, 100) for i in range(3)]\n",
    "best_feature_sets = [get_top_features(feature_names, ols, i, 100, bottom=True) for i in range(3)]\n",
    "\n",
    "# reduce the independent feature sets to just the ones\n",
    "# that we see in common across the per-level models (*, **, ***)\n",
    "worst = reduce(np.intersect1d, best_feature_sets)\n",
    "best = reduce(np.intersect1d, worst_feature_sets)\n",
    "\n",
    "# display as a pretty table\n",
    "html_fmt = \"<table><th>More Violations</th><th>Fewer Violations</th><tbody>{}</tbody></table>\"\n",
    "table_rows = [\"<tr><td>{}</td><td>{}</td></tr>\".format(w, b) for w, b in zip(worst, best)]\n",
    "table_body = \"\\n\".join(table_rows)\n",
    "display.HTML(html_fmt.format(table_body))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Takeaways and Recommended Actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Next Steps and Future Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (learn-env)",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "207px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
